#!/bin/bash
set -e

# Script para configurar Ollama y descargar modelos recomendados

echo "ðŸ¤– Configurando Ollama..."
echo ""

# Verificar que Ollama estÃ© corriendo
echo "ðŸ“¡ Verificando conexiÃ³n a Ollama..."
if ! curl -s http://ollama:11434/api/tags > /dev/null 2>&1; then
    echo "âŒ Error: Ollama no estÃ¡ disponible"
    echo "   AsegÃºrate de que el contenedor estÃ© corriendo:"
    echo "   docker compose ps ollama"
    exit 1
fi

echo "âœ… Ollama estÃ¡ disponible"
echo ""

# FunciÃ³n para pull de modelo
pull_model() {
    local model=$1
    local description=$2
    
    echo "ðŸ“¥ Descargando modelo: $model"
    echo "   $description"
    
    if curl -s -X POST http://ollama:11434/api/pull \
        -d "{\"name\": \"$model\"}" \
        --max-time 600 > /dev/null 2>&1; then
        echo "âœ… $model descargado"
    else
        echo "âš ï¸  Error descargando $model (puede que ya estÃ© instalado)"
    fi
    echo ""
}

# Listar modelos disponibles
echo "ðŸ“‹ Modelos actualmente instalados:"
curl -s http://ollama:11434/api/tags | python3 -m json.tool 2>/dev/null || echo "Ninguno"
echo ""

# Preguntar quÃ© modelos descargar
echo "ðŸŽ¯ Modelos recomendados para SDD:"
echo ""
echo "1. llama3.2:latest (8B) - General purpose, rÃ¡pido"
echo "2. codellama:latest (7B) - Especializado en cÃ³digo"
echo "3. mistral:latest (7B) - Bueno en razonamiento"
echo "4. qwen2.5-coder:latest (7B) - Excelente para cÃ³digo"
echo "5. deepseek-coder:latest (6.7B) - Muy bueno para cÃ³digo"
echo ""

read -p "Â¿Descargar modelo por defecto (llama3.2:latest)? [Y/n] " -n 1 -r
echo ""

if [[ $REPLY =~ ^[Yy]$ ]] || [[ -z $REPLY ]]; then
    pull_model "llama3.2:latest" "General purpose, rÃ¡pido (8B)"
fi

read -p "Â¿Descargar modelo especializado en cÃ³digo (qwen2.5-coder:latest)? [y/N] " -n 1 -r
echo ""

if [[ $REPLY =~ ^[Yy]$ ]]; then
    pull_model "qwen2.5-coder:latest" "Excelente para cÃ³digo (7B)"
fi

# Verificar modelos instalados
echo "ðŸ“‹ Modelos instalados despuÃ©s de setup:"
curl -s http://ollama:11434/api/tags | python3 -c "
import sys, json
data = json.load(sys.stdin)
models = data.get('models', [])
if models:
    for model in models:
        name = model.get('name', 'unknown')
        size = model.get('size', 0) / (1024**3)  # GB
        print(f'  - {name} ({size:.1f} GB)')
else:
    print('  (ninguno)')
" 2>/dev/null || echo "  (error listando modelos)"

echo ""
echo "âœ… Setup de Ollama completado!"
echo ""
echo "ðŸ’¡ Para usar Ollama en tu cÃ³digo:"
echo "   from src.utils.ollama_client import get_llm_router"
echo "   router = get_llm_router()"
echo "   response = router.generate('Tu prompt aquÃ­')"
echo ""
echo "ðŸ’¡ Para cambiar modelo por defecto:"
echo "   Edita OLLAMA_MODEL en .env"
echo ""
echo "ðŸ’¡ Para descargar mÃ¡s modelos manualmente:"
echo "   docker compose exec ollama ollama pull <modelo>"
echo ""
